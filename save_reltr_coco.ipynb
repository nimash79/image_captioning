{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2026 Nima Sharifinia\n",
    "# Licensed under the Apache License, Version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/nima/Downloads/\"\n",
    "features_type = \"gat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.nn import GCNConv , GAT\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv  # Importing SAGEConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, num_words=None, oov_token=\"<unk>\"):\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "        self.max_length = 0\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # Convert to lowercase and remove punctuation (except spaces)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~]', '', text)\n",
    "        return text\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        word_counts = {}\n",
    "\n",
    "        # Split sentences into words correctly\n",
    "        for text in texts:\n",
    "            cleaned_text = self.clean_text(text)  # Proper cleaning\n",
    "            words = cleaned_text.split()  # Split sentence into words\n",
    "            if self.max_length < len(words):\n",
    "              self.max_length = len(words)\n",
    "            for word in words:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "        # Sort words by frequency\n",
    "        sorted_vocab = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Limit vocab to num_words (excluding OOV)\n",
    "        vocab_size = self.num_words - 1 if self.num_words else len(sorted_vocab)\n",
    "\n",
    "        # Build the word_to_idx and idx_to_word dictionaries\n",
    "        self.word_to_idx = {\n",
    "            \"<pad>\": 0,\n",
    "            \"<unk>\": 1,\n",
    "        }\n",
    "        self.idx_to_word = {\n",
    "            0: \"<pad>\",\n",
    "            1: \"<unk>\",\n",
    "        }\n",
    "\n",
    "        for idx, (word, _) in enumerate(sorted_vocab[:vocab_size], start=2):\n",
    "            self.word_to_idx[word] = idx\n",
    "            self.idx_to_word[idx] = word\n",
    "\n",
    "    def text_to_sequence(self, text):\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        words = cleaned_text.split()  # Ensure proper tokenization\n",
    "        return [self.word_to_idx.get(word, self.word_to_idx[\"<unk>\"]) for word in words]  # Map to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44624593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataSet(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.all_captions = []\n",
    "        self.all_images = []\n",
    "        self.all_ids = []\n",
    "        self.max_length = 0\n",
    "        self.PATH = f\"{path}/coco2017/train2017/\"\n",
    "\n",
    "        # Load annotations\n",
    "        with open(f'{path}/coco2017/annotations/captions_train2017.json', 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Create an image index\n",
    "        image_id_index = {}\n",
    "        for img in data['images']:\n",
    "            image_id_index[img['id']] = img['file_name']\n",
    "\n",
    "        for annot in data['annotations']:\n",
    "            caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "            image_id = annot['image_id']\n",
    "            full_coco_image_path = self.PATH + image_id_index[image_id]\n",
    "            if not image_id in self.all_ids:\n",
    "                self.all_ids.append(image_id)\n",
    "                self.all_images.append(full_coco_image_path)\n",
    "                self.all_captions.append(caption)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.all_captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetches the image and encoded caption at the specified index.\"\"\"\n",
    "        image_name = self.all_images[idx]\n",
    "\n",
    "        image = Image.open(image_name)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image_name, image\n",
    "\n",
    "# Example usage\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),  # Ensure 3 channels (convert grayscale to RGB)\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CocoDataSet(transform=transform)\n",
    "dataset_len = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eda6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoValDataSet(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "        self.all_captions = []\n",
    "        self.all_images = []\n",
    "        self.all_ids = []\n",
    "        self.max_length = 0\n",
    "        self.PATH = f\"{path}/coco2017/val2017/\"\n",
    "\n",
    "        # Load annotations\n",
    "        with open(f'{path}/coco2017/annotations/captions_val2017.json', 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Create an image index\n",
    "        image_id_index = {}\n",
    "        for img in data['images']:\n",
    "            image_id_index[img['id']] = img['file_name']\n",
    "\n",
    "        for annot in data['annotations']:\n",
    "            caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "            image_id = annot['image_id']\n",
    "            if not image_id in self.all_ids:\n",
    "              full_coco_image_path = self.PATH + image_id_index[image_id]\n",
    "              self.all_ids.append(image_id)\n",
    "              self.all_images.append(full_coco_image_path)\n",
    "              self.all_captions.append(caption)\n",
    "\n",
    "    def encode(self, caption):\n",
    "        caption = tokenizer.clean_text(caption)\n",
    "        encoded_caption = tokenizer.text_to_sequence(caption)\n",
    "        remain = tokenizer.max_length - len(encoded_caption)\n",
    "        for i in range(remain):\n",
    "            encoded_caption.append(0)\n",
    "\n",
    "        return torch.tensor(encoded_caption)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.all_captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetches the image and one-hot encoded caption at the specified index.\"\"\"\n",
    "        image_name = self.all_images[idx]\n",
    "\n",
    "        image = Image.open(image_name)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image_name, image\n",
    "\n",
    "\n",
    "# Example usage\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),  # Ensure 3 channels (convert grayscale to RGB)\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the custom dataset\n",
    "val_dataset = CocoValDataSet(transform=transform)\n",
    "val_dataset_len = len(val_dataset)\n",
    "tokenizer = CustomTokenizer(num_words=5000, oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(val_dataset.all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [ 'NA', 'airplane', 'animal', 'arm', 'bag', 'banana', 'basket', 'beach', 'bear', 'bed', 'bench', 'bike',\n",
    "                'bird', 'board', 'boat', 'book', 'boot', 'bottle', 'bowl', 'box', 'boy', 'branch', 'building',\n",
    "                'bus', 'cabinet', 'cap', 'car', 'cat', 'chair', 'child', 'clock', 'coat', 'counter', 'cow', 'cup',\n",
    "                'curtain', 'desk', 'dog', 'door', 'drawer', 'ear', 'elephant', 'engine', 'eye', 'face', 'fence',\n",
    "                'finger', 'flag', 'flower', 'food', 'fork', 'fruit', 'giraffe', 'girl', 'glass', 'glove', 'guy',\n",
    "                'hair', 'hand', 'handle', 'hat', 'head', 'helmet', 'hill', 'horse', 'house', 'jacket', 'jean',\n",
    "                'kid', 'kite', 'lady', 'lamp', 'laptop', 'leaf', 'leg', 'letter', 'light', 'logo', 'man', 'men',\n",
    "                'motorcycle', 'mountain', 'mouth', 'neck', 'nose', 'number', 'orange', 'pant', 'paper', 'paw',\n",
    "                'people', 'person', 'phone', 'pillow', 'pizza', 'plane', 'plant', 'plate', 'player', 'pole', 'post',\n",
    "                'pot', 'racket', 'railing', 'rock', 'roof', 'room', 'screen', 'seat', 'sheep', 'shelf', 'shirt',\n",
    "                'shoe', 'short', 'sidewalk', 'sign', 'sink', 'skateboard', 'ski', 'skier', 'sneaker', 'snow',\n",
    "                'sock', 'stand', 'street', 'surfboard', 'table', 'tail', 'tie', 'tile', 'tire', 'toilet', 'towel',\n",
    "                'tower', 'track', 'train', 'tree', 'truck', 'trunk', 'umbrella', 'vase', 'vegetable', 'vehicle',\n",
    "                'wave', 'wheel', 'window', 'windshield', 'wing', 'wire', 'woman', 'zebra']\n",
    "\n",
    "REL_CLASSES = ['background', 'above', 'across', 'against', 'along', 'and', 'at', 'attached to', 'behind',\n",
    "                'belonging to', 'between', 'carrying', 'covered in', 'covering', 'eating', 'flying in', 'for',\n",
    "                'from', 'growing on', 'hanging from', 'has', 'holding', 'in', 'in front of', 'laying on',\n",
    "                'looking at', 'lying on', 'made of', 'mounted on', 'near', 'of', 'on', 'on back of', 'over',\n",
    "                'painted on', 'parked on', 'part of', 'playing', 'riding', 'says', 'sitting on', 'standing on',\n",
    "                'to', 'under', 'using', 'walking in', 'walking on', 'watching', 'wearing', 'wears', 'with']\n",
    "\n",
    "\n",
    "from models.backbone import Backbone, Joiner\n",
    "from models.position_encoding import PositionEmbeddingSine\n",
    "from models.transformer import Transformer\n",
    "from models.reltr import RelTR\n",
    "\n",
    "position_embedding = PositionEmbeddingSine(128, normalize=True)\n",
    "backbone = Backbone('resnet50', False, False, False)\n",
    "backbone = Joiner(backbone, position_embedding)\n",
    "backbone.num_channels = 2048\n",
    "\n",
    "transformer = Transformer(d_model=256, dropout=0.3, nhead=8,\n",
    "                          dim_feedforward=2048,\n",
    "                          num_encoder_layers=6,\n",
    "                          num_decoder_layers=6,\n",
    "                          normalize_before=False,\n",
    "                          return_intermediate_dec=True)\n",
    "\n",
    "rel_tr_model = RelTR(backbone, transformer, num_classes=151, num_rel_classes = 51,\n",
    "              num_entities=100, num_triplets=200)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# The checkpoint is pretrained on Visual Genome\n",
    "ckpt = torch.load(\n",
    "    'checkpoint_reltr.pth',\n",
    "    map_location=device,\n",
    "    weights_only=False)\n",
    "\n",
    "for param in rel_tr_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "reltr_model = rel_tr_model.to(device)\n",
    "reltr_model.load_state_dict(ckpt['model'])\n",
    "reltr_model.eval()\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "          (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "\n",
    "coded_classes = []\n",
    "coded_rels = []\n",
    "\n",
    "for cls in CLASSES:\n",
    "    cleaned_cls = tokenizer.clean_text(cls)\n",
    "    words = cleaned_cls.split(\" \")\n",
    "    coded_words = []\n",
    "    for w in words:\n",
    "      coded_words.append(tokenizer.word_to_idx.get(w, tokenizer.word_to_idx[\"<unk>\"]))\n",
    "    coded_classes.append(coded_words)\n",
    "\n",
    "max_len = 0\n",
    "for cls in REL_CLASSES:\n",
    "    cleaned_cls = tokenizer.clean_text(cls)\n",
    "    words = cleaned_cls.split(\" \")\n",
    "    coded_words = []\n",
    "    for w in words:\n",
    "      coded_words.append(tokenizer.word_to_idx.get(w, tokenizer.word_to_idx[\"<unk>\"]))\n",
    "    coded_rels.append(coded_words)\n",
    "    if len(words) > max_len:\n",
    "      max_len = len(words)\n",
    "\n",
    "# Max length of rel classes = max_len\n",
    "# Now make the length of rel classes same as max_len (padding)\n",
    "padded_tensors = []\n",
    "for t in coded_rels:\n",
    "    # اگر طول تنسور کمتر از max_len است، آن را با صفر پد می‌کنیم\n",
    "    padding = max_len - len(t)\n",
    "    for i in range(padding):\n",
    "        t.append(0)\n",
    "    padded_tensors.append(t)\n",
    "\n",
    "padded_tensors = torch.tensor(padded_tensors)\n",
    "coded_classes = torch.tensor(coded_classes)\n",
    "coded_rels = torch.tensor(padded_tensors)\n",
    "\n",
    "\n",
    "transform_reltr = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "class ModifiedReltr(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "      self.rel_tr_model = rel_tr_model.to(device)\n",
    "      self.rel_tr_model.load_state_dict(ckpt['model'])\n",
    "      self.rel_tr_model.eval()\n",
    "\n",
    "  def forward(self, x):\n",
    "    outputs_all = []\n",
    "    for im in x:\n",
    "      img = transform_reltr(im).unsqueeze(0)\n",
    "      # propagate through the model\n",
    "      outputs = rel_tr_model(img)\n",
    "      # keep only predictions with >0.3 confidence\n",
    "      probas = outputs['rel_logits'].softmax(-1)[0, :, :-1]\n",
    "      probas_sub = outputs['sub_logits'].softmax(-1)[0, :, :-1]\n",
    "      probas_obj = outputs['obj_logits'].softmax(-1)[0, :, :-1]\n",
    "      keep = torch.logical_and(probas.max(-1).values > 0.3, torch.logical_and(probas_sub.max(-1).values > 0.3,\n",
    "                                                                              probas_obj.max(-1).values > 0.3))\n",
    "      filtered_probas = probas[keep]\n",
    "      filtered_probas_sub = probas_sub[keep]\n",
    "      filtered_probas_obj = probas_obj[keep]\n",
    "\n",
    "      nodes = []\n",
    "      node_ids = []\n",
    "      # return nodes\n",
    "      edges = [[],[]]\n",
    "      edge_attr = []\n",
    "      for idx, _ in enumerate(filtered_probas):\n",
    "        obj = coded_classes[filtered_probas_obj[idx].argmax()]\n",
    "        obj_id = filtered_probas_obj[idx].argmax()\n",
    "        if not obj_id in node_ids:\n",
    "          nodes.append(obj)\n",
    "          node_ids.append(obj_id)\n",
    "        sub = coded_classes[filtered_probas_sub[idx].argmax()]\n",
    "        sub_id = filtered_probas_sub[idx].argmax()\n",
    "        if not sub_id in node_ids:\n",
    "          nodes.append(sub)\n",
    "          node_ids.append(sub_id)\n",
    "\n",
    "\n",
    "        # print(CLASSES[filtered_probas_obj[idx].argmax()])\n",
    "        idx_obj = node_ids.index(filtered_probas_obj[idx].argmax())\n",
    "        idx_sub = node_ids.index(filtered_probas_sub[idx].argmax())\n",
    "        edges[0].append(idx_sub)\n",
    "        edges[1].append(idx_obj)\n",
    "        edge_attr.append(padded_tensors[filtered_probas[idx].argmax()])\n",
    "\n",
    "      if (len(nodes)>0):\n",
    "        np_array1 = torch.stack(nodes)\n",
    "        np_array1 = np_array1.to(device,dtype=torch.float)\n",
    "        # print(np_array1.shape)\n",
    "        x_np1 = np_array1.reshape((np_array1.shape[0],1))\n",
    "      else :\n",
    "        x_np1 = torch.tensor([[]]).to(device)\n",
    "      edges_new = [torch.from_numpy(np.array(e)) for e in edges]\n",
    "      np_array2 = torch.stack(edges_new)\n",
    "      x_np2 = np_array2.to(device,dtype=int)\n",
    "\n",
    "\n",
    "\n",
    "      if (len(nodes)>0):\n",
    "        x_np3 = torch.stack(edge_attr).to(device)\n",
    "      else :\n",
    "        x_np3 = torch.tensor([]).to(device)\n",
    "\n",
    "\n",
    "      graph_data = Data(x=x_np1, edge_index=x_np2,edge_attr=x_np3)\n",
    "      outputs_all.append(graph_data)\n",
    "\n",
    "\n",
    "    return outputs_all\n",
    "  \n",
    "\n",
    "class MainGCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MainGCN, self).__init__()\n",
    "        self.conv1 = GraphConv(input_dim, hidden_dim)  # Using GraphConv\n",
    "        self.conv2 = GraphConv(hidden_dim, output_dim)  # Using GraphConv\n",
    "        self.fc = nn.Linear(output_dim, 2048)\n",
    "\n",
    "    def forward(self, ls):\n",
    "      out = []\n",
    "      for g in ls:\n",
    "        # print(g)\n",
    "        x , edge_index , edge_attr = g.x , g.edge_index,g.edge_attr\n",
    "        if(x.size()==(1, 0)):\n",
    "          out.append(torch.zeros((1,2048)).to(device))\n",
    "          continue\n",
    "        # x = self.conv1(x, edge_index,edge_attr)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # x = self.conv2(x, edge_index,edge_attr)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, g.batch)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        out.append(x)\n",
    "      return torch.stack(out)\n",
    "\n",
    "class MainGAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=4):\n",
    "        super(MainGAT, self).__init__()\n",
    "        self.att1 = GAT(input_dim, hidden_dim,num_layers)  # Using GraphConv\n",
    "        self.att2 = GAT(hidden_dim, output_dim,num_layers)  # Using GraphConv\n",
    "        self.fc = nn.Linear(output_dim, 2048)\n",
    "\n",
    "    def forward(self, ls):\n",
    "      out = []\n",
    "      for g in ls:\n",
    "        # print(g)\n",
    "        x , edge_index , edge_attr = g.x , g.edge_index,g.edge_attr\n",
    "        if(x.size()==(1, 0)):\n",
    "          out.append(torch.zeros((1,2048)).to(device))\n",
    "          continue\n",
    "        # x = self.conv1(x, edge_index,edge_attr)\n",
    "        x = self.att1(x, edge_index,edge_attr=edge_attr)\n",
    "        x = F.relu(x)\n",
    "        # x = self.conv2(x, edge_index,edge_attr)\n",
    "        x = self.att2(x, edge_index,edge_attr=edge_attr)\n",
    "\n",
    "        x = global_mean_pool(x, g.batch)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        out.append(x)\n",
    "      return torch.stack(out)\n",
    "\n",
    "relTR_model = ModifiedReltr().to(device)\n",
    "if features_type == \"gcn\":\n",
    "  graph_encoder = MainGCN(input_dim=-1, hidden_dim=4, output_dim=2048).to(device)\n",
    "else:\n",
    "  graph_encoder = MainGAT(input_dim=-1, hidden_dim=4, output_dim=2048).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308df7c",
   "metadata": {},
   "source": [
    "### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e134b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Dataset\")\n",
    "for i, (img_name, img_tensor) in enumerate(dataset):\n",
    "    print(f\"{i}/{dataset_len}\")\n",
    "    graphs = relTR_model(torch.tensor(img_tensor).unsqueeze(0).to(device))\n",
    "    graph_features = graph_encoder(graphs)\n",
    "    graph_features = graph_features.detach().cpu().numpy()\n",
    "    np.save(img_name + \"_reltr\" + features_type, graph_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e603d",
   "metadata": {},
   "source": [
    "### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Dataset\")\n",
    "for i, (img_name, img_tensor) in enumerate(val_dataset):\n",
    "    print(f\"{i}/{val_dataset_len}\")\n",
    "    graphs = relTR_model(torch.tensor(img_tensor).unsqueeze(0).to(device))\n",
    "    graph_features = graph_encoder(graphs)\n",
    "    graph_features = graph_features.detach().cpu().numpy()\n",
    "    np.save(img_name + \"_reltr_\" + features_type, graph_features)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
